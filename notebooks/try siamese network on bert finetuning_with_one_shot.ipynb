{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/advice/notebook/jms/우리은행/pytorch-pretrained-BERT/examples/')\n",
    "sys.path.append('/home/advice/notebook/jms/우리은행/pytorch-pretrained-BERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_squad_spm import *\n",
    "from  pytorch_pretrained_bert import modeling\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "today = str(today).replace('-', '')\n",
    "direc = '/home/advice/notebook/jms/우리은행/output_dir/'\n",
    "try:\n",
    "    os.mkdir(direc+today)\n",
    "    print(direc , today ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(direc , today ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import pdb\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from pytorch_pretrained_bert.optimization import BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/advice/notebook/jms/우리은행/extract_kobert/\"\n",
    "\n",
    "vocab_file = '/home/advice/notebook/jms/kobert/kobert_news_wiki_ko_cased-1087f8699e.spiece'\n",
    "bert_config_file = model_dir + \"kobert_config.json\"\n",
    "init_checkpoint = model_dir + \"kobert_model.bin\"\n",
    "bert_model = 'kobert'\n",
    "PATH = \"/home/advice/notebook/jms/우리은행/data/\"\n",
    "\n",
    "output_dir = direc+today+'/'\n",
    "train_file = \"news_tr_small_5.txt\"\n",
    "eval_file = \"news_te.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"train_file\": train_file,\n",
    "    \"eval_file\": eval_file,\n",
    "    \"data_dir\": PATH,\n",
    "    \"task_name\": \"news_multilabel\",\n",
    "    \"no_cuda\": False,\n",
    "    \"bert_model\": model_dir,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"tokenizer\": vocab_file,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\": 32,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"num_train_epochs\": 4.0,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimize_on_cpu\": False,\n",
    "    \"fp16\": False,\n",
    "    \"loss_scale\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BERTSPMTokenizer.from_pretrained(args['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/advice/notebook/jms/우리은행/data/news_tr_small_1.txt', \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n",
    "    lines = []\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "tokenizer.tokenize(lines[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer, WordpieceTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining, BertPreTrainedModel, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgVec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgVec, self).__init__()\n",
    "\n",
    "    def forward(self, hidden_states, mask):\n",
    "        last_layer = hidden_states[-1]\n",
    "        input_vecs = last_layer\n",
    "\n",
    "        sum_vecs = (input_vecs * mask.unsqueeze(-1)).sum(1)\n",
    "        avg_vecs = sum_vecs / mask.sum(1, keepdim=True)\n",
    "        return avg_vecs\n",
    "\n",
    "\n",
    "class BertForSiameseClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForSiameseClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, 2)\n",
    "        self.apply(self.init_bert_weights)\n",
    "        self.avg_vec = AvgVec()\n",
    "\n",
    "    def forward(self, input_ids_1, input_mask_1, input_ids_2, input_mask_2):\n",
    "        self.bert.eval()\n",
    "        encoder_layer_1, pooled_output_1 = self.bert(input_ids_1, token_type_ids=None, attention_mask=input_mask_1)\n",
    "        encoder_layer_2, pooled_output_2 = self.bert(input_ids_2, token_type_ids=None, attention_mask=input_mask_2)\n",
    "        out1 = self.avg_vec(encoder_layer_1, input_mask_1)\n",
    "        out2 = self.avg_vec(encoder_layer_2, input_mask_2)\n",
    "        out_norm = diff = torch.abs(out1 - out2)\n",
    "        logit = self.classifier(out_norm)\n",
    "        softmax = F.softmax(logit, dim=1)\n",
    "        return logit, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError() \n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/advice/notebook/jms/우리은행/data/news_tr.txt', \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n",
    "    lines = []\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "\n",
    "class MultiLabelTextProcessor(DataProcessor):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = None\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines    \n",
    "    \n",
    "    def get_train_examples(self, data_dir, size=-1):\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, args['train_file'])))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, args['train_file'])), \"train\")\n",
    "        \n",
    "    def get_dev_examples(self, data_dir, size=-1):\n",
    "        filename = 'news_te.txt'\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, args['eval_file'])))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, filename)), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, args['eval_file'])))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, args['eval_file'])), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if self.labels == None:\n",
    "            self.labels = ['0', '1', '2', '3', '4', '5']\n",
    "        return self.labels\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[0]\n",
    "            text_b = None\n",
    "            label = line[1]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, labels=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = []\n",
    "        for label in example.labels:\n",
    "            labels_ids.append(float(label))\n",
    "\n",
    "#         label_id = label_map[example.label]\n",
    "        if ex_index < 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    out_cpu = out.cpu().numpy()\n",
    "    labels_cpu = labels.cpu().numpy()\n",
    "    outputs = np.argmax(out_cpu, axis=1)\n",
    "    return np.sum(outputs == labels_cpu)\n",
    "\n",
    "def accuracy_thresh(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=True):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "#     return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n",
    "    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()\n",
    "\n",
    "\n",
    "def fbeta(y_pred:Tensor, y_true:Tensor, thresh:float=0.2, beta:float=2, eps:float=1e-9, sigmoid:bool=True):\n",
    "    \"Computes the f_beta between `preds` and `targets`\"\n",
    "    beta2 = beta ** 2\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "    y_pred = (y_pred>thresh).float()\n",
    "    y_true = y_true.float()\n",
    "    TP = (y_pred*y_true).sum(dim=1)\n",
    "    prec = TP/(y_pred.sum(dim=1)+eps)\n",
    "    rec = TP/(y_true.sum(dim=1)+eps)\n",
    "    res = (prec*rec)/(prec*beta2+rec+eps)*(1+beta2)\n",
    "    return res.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"news_multilabel\": MultiLabelTextProcessor\n",
    "}\n",
    "\n",
    "# Setup GPU parameters\n",
    "\n",
    "if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "#     n_gpu = 1\n",
    "else:\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "    device = torch.device(\"cuda\", args['local_rank'])\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args['local_rank'] != -1), args['fp16']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = args['task_name'].lower()\n",
    "if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "    \n",
    "processor = processors[task_name](args['data_dir'])\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BERTSPMTokenizer.from_pretrained(args['tokenizer'])\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if args['do_train']:\n",
    "    train_examples = processor.get_train_examples(args['data_dir'], size=args['train_size'])\n",
    "#     train_examples = processor.get_train_examples(args['data_dir'], size=args['train_size'])\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / args['train_batch_size'] / args['gradient_accumulation_steps'] * args['num_train_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, args['max_seq_length'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "model = BertForSiameseClassification(bert_config, num_labels = num_labels)\n",
    "model.bert.load_state_dict(torch.load(init_checkpoint))\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self,feature_id, data_a, data_a_mask, data_a_lab,\n",
    "                 data_b, data_b_mask, data_b_lab, label):\n",
    "        self.feature_id = feature_id\n",
    "        self.data_a = data_a\n",
    "        self.data_a_mask = data_a_mask\n",
    "        self.data_a_lab = data_a_lab\n",
    "        self.data_b = data_b\n",
    "        self.data_b_mask = data_b_mask\n",
    "        self.data_b_lab = data_b_lab\n",
    "        self.label = label\n",
    "\n",
    "def convert_to_siamese_features(tf):\n",
    "    siamese_features = []\n",
    "    tf_len = len(tf)\n",
    "    for idx in range(tf_len):\n",
    "        data_a  = tf[idx].input_ids\n",
    "        data_a_mask = tf[idx].input_mask\n",
    "        data_a_lab = tf[idx].label_ids\n",
    "        for idx_2 in range(idx, tf_len):\n",
    "            data_b = tf[idx_2].input_ids\n",
    "            data_b_mask = tf[idx_2].input_mask\n",
    "            data_b_lab = tf[idx_2].label_ids\n",
    "            label = int(np.where(tf[idx].label_ids == tf[idx_2].label_ids, \n",
    "                             1,0))\n",
    "            siamese_features.append(SiameseFeatures(feature_id = 'train_{}_{}'.format(int(idx), int(idx_2)), \n",
    "                                                    data_a = data_a, \n",
    "                                                    data_a_mask = data_a_mask, \n",
    "                                                    data_a_lab = data_a_lab, \n",
    "                                                    data_b = data_b, \n",
    "                                                    data_b_mask = data_b_mask, \n",
    "                                                    data_b_lab = data_b_lab, \n",
    "                                                    label = label))\n",
    "    return siamese_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_features = convert_to_siamese_features(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_a = torch.tensor([f.data_a for f in siamese_features],dtype=torch.long).to(device)\n",
    "all_data_a_mask = torch.tensor([f.data_a_mask for f in siamese_features],dtype=torch.float).to(device)\n",
    "all_data_b = torch.tensor([f.data_b for f in siamese_features],dtype=torch.long).to(device)\n",
    "all_data_b_mask = torch.tensor([f.data_b_mask for f in siamese_features],dtype=torch.float).to(device)\n",
    "all_label = torch.tensor([f.label for f in siamese_features]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(all_data_a, all_data_a_mask, all_data_b, all_data_b_mask, all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter=10\n",
    "model.train()\n",
    "\n",
    "optimizer = Adam(model.module.classifier.parameters(), lr=0.1)\n",
    "\n",
    "loss_total = 0.0\n",
    "trial = 0\n",
    "for _ in range(iter) : \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        dat_a, mask_a, dat_b, mask_b, lab= batch\n",
    "        logit, _ = model(dat_a, mask_a, dat_b, mask_b)\n",
    "        loss_func = torch.nn.CrossEntropyLoss() \n",
    "        loss = loss_func(logit, lab)\n",
    "        if n_gpu>1:\n",
    "            loss = loss.mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_total += loss.item()\n",
    "        trial += 1\n",
    "\n",
    "    print(\"loss : \", loss_total/trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_processor = MultiLabelTextProcessor(PATH)\n",
    "test_examples = predict_processor.get_test_examples(PATH, 'news_te.csv', size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = convert_examples_to_features(\n",
    "    test_examples, label_list, args['max_seq_length'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_siamese_features_for_test(test, train):\n",
    "    siamese_features = []\n",
    "    test_len = len(test)\n",
    "    train_len = len(train)\n",
    "    for idx in range(test_len):\n",
    "        data_a  = test[idx].input_ids\n",
    "        data_a_mask = test[idx].input_mask\n",
    "        data_a_lab = test[idx].label_ids\n",
    "        for idx_2 in range(train_len):\n",
    "            data_b = train[idx_2].input_ids\n",
    "            data_b_mask = train[idx_2].input_mask\n",
    "            data_b_lab = train[idx_2].label_ids\n",
    "            label = int(np.where(test[idx].label_ids == train[idx_2].label_ids, \n",
    "                             1,0))\n",
    "            siamese_features.append(SiameseFeatures(feature_id = 'test_{}'.format(int(idx)), \n",
    "                                                    data_a = data_a, \n",
    "                                                    data_a_mask = data_a_mask, \n",
    "                                                    data_a_lab = data_a_lab, \n",
    "                                                    data_b = data_b, \n",
    "                                                    data_b_mask = data_b_mask, \n",
    "                                                    data_b_lab = data_b_lab, \n",
    "                                                    label = label))\n",
    "    return siamese_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = convert_to_siamese_features_for_test(test_features[:100], train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = torch.tensor([f.data_a for f in test_set],dtype=torch.long).to(device)\n",
    "all_test_data_mask = torch.tensor([f.data_a_mask for f in test_set],dtype=torch.float).to(device)\n",
    "all_compare_data = torch.tensor([f.data_b for f in test_set],dtype=torch.long).to(device)\n",
    "all_compare_data_mask = torch.tensor([f.data_b_mask for f in test_set],dtype=torch.float).to(device)\n",
    "test_ans = [f.data_a_lab[0] for f in test_set]\n",
    "test_id = [f.feature_id for f in test_set]\n",
    "train_lab = [f.data_b_lab[0] for f in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab = [f.data_b_lab[0] for f in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(all_test_data, all_test_data_mask, all_compare_data, all_compare_data_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = SequentialSampler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_logits = None\n",
    "for step, batch in enumerate(tqdm(test_dataloader, desc=\"Prediction Iteration\")):\n",
    "            test_data, test_data_mask, compare_data, compare_data_mask = batch\n",
    "            test_data = test_data.to(device)\n",
    "            test_data_mask = test_data_mask.to(device)\n",
    "            compare_data = compare_data.to(device)\n",
    "            compare_data_mask = compare_data_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                _,logits = model(test_data, test_data_mask, compare_data, compare_data_mask)\n",
    "                if all_logits is None:\n",
    "                    all_logits = logits.detach().cpu().numpy()\n",
    "                else:\n",
    "                    all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'test_id': test_id, 'train_label':train_lab, 'prob':all_logits[:,1],'real':test_ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pivot = pd.pivot_table(result, values='prob', \n",
    "                              index = ['test_id', 'real'], \n",
    "                              columns = ['train_label'], aggfunc = np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pivot.loc[:,'pred'] = result_pivot.apply(lambda row: np.argmax(row), axis = 1)\n",
    "result_pivot = result_pivot.reset_index()\n",
    "result_pivot[result_pivot['real'] == result_pivot['pred']].shape[0]/result_pivot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pivot = result_pivot.reset_index()\n",
    "result_pivot[result_pivot['real'] == result_pivot['pred']].shape[0]/result_pivot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pivot[result_pivot['real'] == result_pivot['pred']].shape[0]/result_pivot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pivot[result_pivot['real'] == result_pivot['pred']].shape[0]/result_pivot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([[f.input_ids] for f in train_features],dtype=torch.long).to(device)\n",
    "all_input_mask = torch.tensor([[f.input_mask] for f in train_features],dtype=torch.float).to(device)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in train_features]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_input_ids = [f.input_ids for f in train_features]\n",
    "# all_input_mask = [f.input_mask for f in train_features]\n",
    "# all_label_ids = [f.label_ids for f in train_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []\n",
    "for idx in range(len(train_features)):\n",
    "    tmp = [[[all_input_ids[idx], all_input_mask[idx], all_label_ids[idx]],\n",
    "            [all_input_ids[idx_2], all_input_mask[idx_2], all_label_ids[idx_2]], \n",
    "            int(np.where(idx==idx_2, 1,0))] for idx_2 in range(idx,len(train_features)) ]\n",
    "    data_set.extend(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(data_set)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = torch.tensor([i[0][0]for i in data_set],dtype=torch.long).to(device)\n",
    "a2 = torch.tensor([i[0][1]for i in data_set],dtype=torch.float).to(device)\n",
    "b1 = torch.tensor([i[1][0]for i in data_set],dtype=torch.long).to(device)\n",
    "b2 = torch.tensor([i[1][1]for i in data_set],dtype=torch.float).to(device)\n",
    "lab = torch.tensor([i[2] for i in data_set]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = TensorDataset(a1, a2, b1, b2,lab)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=1\n",
    "# iter=10\n",
    "# model.train()\n",
    "\n",
    "# optimizer = Adam(model.module.classifier.parameters(), lr=0.1)\n",
    "\n",
    "# loss_total = 0.0\n",
    "# trial = 0\n",
    "# for _ in range(iter) : \n",
    "#     print(0)\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         print(1)\n",
    "#         a1,a2,b1,b2,lab= batch\n",
    "#         print(2)\n",
    "#         logit, _ = model(a1,a2,b1,b2)\n",
    "#         print(3)\n",
    "#         loss_func = torch.nn.CrossEntropyLoss() \n",
    "#         loss = loss_func(logit, lab)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         loss_total += loss.item()\n",
    "#         trial += 1\n",
    "\n",
    "#     print(\"loss : \", loss_total/trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "iter=4\n",
    "model.cuda()\n",
    "model.train()\n",
    "\n",
    "optimizer = Adam(model.module.classifier.parameters(), lr=0.1)\n",
    "\n",
    "loss_total = 0.0\n",
    "trial = 0\n",
    "for _ in range(iter) : \n",
    "    for data in data_set:\n",
    "        logit, _ = model(data[0][0], data[0][1], data[1][0], data[1][1])\n",
    "        y  = torch.tensor([data[2]]).cuda()\n",
    "        loss_func = torch.nn.CrossEntropyLoss() \n",
    "        loss = loss_func(logit, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_total += loss.item()\n",
    "        trial += 1\n",
    "\n",
    "    print(\"loss : \", loss_total/trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_processor = MultiLabelTextProcessor(PATH)\n",
    "test_examples = predict_processor.get_test_examples(PATH, 'news_te.csv', size=-1)\n",
    "\n",
    "# Hold input data for returning it \n",
    "input_data = [{ 'id': input_example.guid, 'comment_text': input_example.text_a } for input_example in test_examples]\n",
    "\n",
    "test_features = convert_examples_to_features(\n",
    "    test_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "\n",
    "\n",
    "all_input_ids_test = torch.tensor([[f.input_ids] for f in test_features],dtype=torch.long).to(device)\n",
    "all_input_mask_test = torch.tensor([[f.input_mask] for f in test_features],dtype=torch.float).to(device)\n",
    "ans = [int(f.label_ids[0]) for f in test_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(test_id, test_mask) : \n",
    "    model.eval()\n",
    "    prob_list = []\n",
    "    for idx in range(len(train_features)) :\n",
    "        p = model(test_id, test_mask, all_input_ids[idx], all_input_mask[idx])\n",
    "        prob_list.append(p[1][0][1])\n",
    "#         print(\"Prob of {0} : {1}\".format(idx, p[1][0][1]))\n",
    "#     print('predict: ',np.argmax(prob_list))\n",
    "    return np.argmax(prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_oneshot(all_input_ids_test[3], all_input_mask_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_oneshot(all_input_ids_test[10], all_input_mask_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(len(test_features)):\n",
    "    k = ans[i]==test_oneshot(all_input_ids_test[i], all_input_mask_test[i])\n",
    "    print(ans[i]==test_oneshot(all_input_ids_test[i], all_input_mask_test[i]))\n",
    "    print('real: ',ans[i])\n",
    "    x.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(len(test_features)):\n",
    "    k = ans[i]==test_oneshot(all_input_ids_test[i], all_input_mask_test[i])\n",
    "    print(ans[i]==test_oneshot(all_input_ids_test[i], all_input_mask_test[i]))\n",
    "    print('real: ',ans[i])\n",
    "    x.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", args['train_batch_size'])\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features])#, dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features])#, dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in train_features])#, dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "if args['local_rank'] == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args['train_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.unfreeze_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epocs=args['num_train_epochs']\n",
    "\n",
    "global_step = 0\n",
    "model.train()\n",
    "for i_ in tqdm(range(int(num_epocs)), desc=\"Epoch\"):\n",
    "\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        if args['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "        if args['fp16']:\n",
    "            optimizer.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "#             scheduler.batch_step()\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = args['learning_rate'] * warmup_linear(global_step/t_total, args['warmup_proportion'])\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "    logger.info('Loss after epoc {}'.format(tr_loss / nb_tr_steps))\n",
    "    logger.info('Eval after epoc {}'.format(i_+1))\n",
    "    eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(output_dir, \"finetuned_pytorch_model.bin\")\n",
    "torch.save(model_to_save.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file)\n",
    "model = BertForSiameseClassification.from_pretrained(args['bert_model'], num_labels = num_labels, state_dict=model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, path, test_filename='news_te.csv'):\n",
    "    predict_processor = MultiLabelTextProcessor(path)\n",
    "    test_examples = predict_processor.get_test_examples(path, test_filename, size=-1)\n",
    "    \n",
    "    # Hold input data for returning it \n",
    "    input_data = [{ 'id': input_example.guid, 'comment_text': input_example.text_a } for input_example in test_examples]\n",
    "\n",
    "    test_features = convert_examples_to_features(\n",
    "        test_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "    \n",
    "    logger.info(\"***** Running prediction *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_examples))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "    all_logits = None\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for step, batch in enumerate(tqdm(test_dataloader, desc=\"Prediction Iteration\")):\n",
    "        input_ids, input_mask, segment_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "            logits = logits.sigmoid()\n",
    "\n",
    "        if all_logits is None:\n",
    "            all_logits = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    return pd.merge(pd.DataFrame(input_data), pd.DataFrame(all_logits, columns=label_list), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predict(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.loc[:, 'pred'] = a.iloc[:,2:].apply(lambda x: x.idxmax(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/advice/notebook/jms/우리은행/data/news_te.txt', \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n",
    "    lines = []\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "t = [i[1] for i in lines[1:]]\n",
    "a.loc[:,'real'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a.pred == a.real].shape[0]/a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epocs=args['num_train_epochs']):\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for i_ in tqdm(range(int(num_epocs)), desc=\"Epoch\"):\n",
    "\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "    #             scheduler.batch_step()\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = args['learning_rate'] * warmup_linear(global_step/t_total, args['warmup_proportion'])\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        logger.info('Loss after epoc {}'.format(tr_loss / nb_tr_steps))\n",
    "        logger.info('Eval after epoc {}'.format(i_+1))\n",
    "        eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
