{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/advice/notebook/jms/우리은행/pytorch-pretrained-BERT/examples/')\n",
    "sys.path.append('/home/advice/notebook/jms/우리은행/pytorch-pretrained-BERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pytorch_pretrained_bert import modeling\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/advice/notebook/jms/우리은행/output_dir/ 20200116  already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "today = str(today).replace('-', '')\n",
    "direc = '/home/advice/notebook/jms/우리은행/output_dir/'\n",
    "try:\n",
    "    os.mkdir(direc+today)\n",
    "    print(direc , today ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(direc , today ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union, IO, Callable, Set\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached_path(url_or_filename: Union[str, Path], cache_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Given something that might be a URL (or might be a local path),\n",
    "    determine which. If it's a URL, download the file and cache it, and\n",
    "    return the path to the cached file. If it's already a local path,\n",
    "    make sure the file exists and then return the path.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n",
    "    if isinstance(url_or_filename, Path):\n",
    "        url_or_filename = str(url_or_filename)\n",
    "\n",
    "    parsed = urlparse(url_or_filename)\n",
    "\n",
    "    if parsed.scheme in ('http', 'https', 's3'):\n",
    "        # URL, so get it from the cache (downloading if necessary)\n",
    "        return get_from_cache(url_or_filename, cache_dir)\n",
    "    elif os.path.exists(url_or_filename):\n",
    "        # File, and it exists.\n",
    "        return url_or_filename\n",
    "    elif parsed.scheme == '':\n",
    "        # File, but it doesn't exist.\n",
    "        raise FileNotFoundError(\"file {} not found\".format(url_or_filename))\n",
    "    else:\n",
    "        # Something unknown\n",
    "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "from io import open\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "\n",
    "VOCAB_FILES_NAMES = 'model.piece'\n",
    "class BERTSPMTokenizer(object):\n",
    "    \n",
    "    def __init__(self, vocab_file,do_lower_case=False):\n",
    "\n",
    "#         self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n",
    "#         self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use SPMTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.vocab_file = vocab_file\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        sen = self.sp_model.encode_as_pieces(text)\n",
    "        return [token for token in sen if token!='▁']\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.sp_model.piece_to_id(token))\n",
    "        return ids\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.sp_model.decode_ids(i))\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return ''.join(token).replace('▁', ' ').strip()\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\"You should not supply a second sequence if the provided sequence of \"\n",
    "                                 \"ids is already formated with special tokens for the model.\")\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A BERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "            to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "        out_vocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES)\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n",
    "            copyfile(self.vocab_file, out_vocab_file)\n",
    "\n",
    "        return (out_vocab_file,)\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name, cache_dir=None, *inputs, **kwargs):\n",
    "        if pretrained_model_name in PRETRAINED_VOCAB_ARCHIVE_MAP:\n",
    "            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name]\n",
    "        else:\n",
    "            vocab_file = pretrained_model_name\n",
    "        if os.path.isdir(vocab_file):\n",
    "            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n",
    "        # redirect to the cache, if necessary\n",
    "        try:\n",
    "            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "        # Instantiate tokenizer.\n",
    "        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import pdb\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from pytorch_pretrained_bert.optimization import BertAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/advice/notebook/jms/우리은행/extract_kobert/\"\n",
    "\n",
    "vocab_file = '/home/advice/notebook/jms/kobert/kobert_news_wiki_ko_cased-1087f8699e.spiece'\n",
    "bert_config_file = model_dir + \"kobert_config.json\"\n",
    "init_checkpoint = model_dir + \"kobert_model.bin\"\n",
    "bert_model = 'kobert'\n",
    "PATH = \"/home/advice/notebook/jms/우리은행/data/\"\n",
    "\n",
    "output_dir = direc+today+'/'\n",
    "train_file = \"news_tr.txt\"\n",
    "eval_file = \"news_te.txt\"\n",
    "dev_file = 'news_tr_small_5.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"train_file\": train_file,\n",
    "    \"eval_file\": eval_file,\n",
    "    \"dev_file\": dev_file,\n",
    "    \"data_dir\": PATH,\n",
    "    \"task_name\": \"news_multilabel\",\n",
    "    \"no_cuda\": False,\n",
    "    \"bert_model\": model_dir,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"tokenizer\": vocab_file,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\": 32,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"num_train_epochs\": 4.0,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimize_on_cpu\": False,\n",
    "    \"fp16\": False,\n",
    "    \"loss_scale\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BERTSPMTokenizer(args['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError() \n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/advice/notebook/jms/우리은행/data/news_tr.txt', \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n",
    "    lines = []\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "\n",
    "class MultiLabelTextProcessor(DataProcessor):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = None\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines    \n",
    "    \n",
    "    def get_train_examples(self, data_dir, size=-1):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, args['train_file'])), \"train\")\n",
    "        \n",
    "    def get_dev_examples(self, data_dir, size=-1):\n",
    "        filename = 'news_te.txt'\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, args['dev_file'])), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, args['eval_file'])), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if self.labels == None:\n",
    "            self.labels = ['0', '1', '2', '3', '4', '5']\n",
    "        return self.labels\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[0]\n",
    "            text_b = None\n",
    "            label = line[1]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, labels=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = []\n",
    "        for label in example.labels:\n",
    "            labels_ids.append(float(label))\n",
    "\n",
    "#         label_id = label_map[example.label]\n",
    "        if ex_index < 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"news_multilabel\": MultiLabelTextProcessor\n",
    "}\n",
    "\n",
    "# Setup GPU parameters\n",
    "\n",
    "if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "#     n_gpu = 1\n",
    "else:\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "    device = torch.device(\"cuda\", args['local_rank'])\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = args['task_name'].lower()\n",
    "if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "    \n",
    "processor = processors[task_name](args['data_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if args['do_train']:\n",
    "    train_examples = processor.get_train_examples(args['data_dir'], size=args['train_size'])\n",
    "#     train_examples = processor.get_train_examples(args['data_dir'], size=args['train_size'])\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / args['train_batch_size'] / args['gradient_accumulation_steps'] * args['num_train_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features_multi_pipe(examples, \n",
    "                                            start, end, \n",
    "                                            max_seq_length, \n",
    "                                            tokenizer, \n",
    "                                            result):\n",
    "\n",
    "    for example in examples[start:end]:\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = []\n",
    "        for label in example.labels:\n",
    "            labels_ids.append(float(label))\n",
    "\n",
    "        result.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "def convert_examples_to_features_multi(train_examples, \n",
    "                                       max_seq_length, \n",
    "                                       tokenizer, \n",
    "                                       n_cpu=6):\n",
    "    _manager = multiprocessing.Manager()\n",
    "    _result = _manager.list()\n",
    "    num_train_examples = len(train_examples)\n",
    "    batch_size = num_train_examples// n_cpu\n",
    "\n",
    "    processes = []\n",
    "    for i in range(n_cpu):\n",
    "        pr = multiprocessing.Process(\n",
    "            target = convert_examples_to_features_multi_pipe, \n",
    "            args = (train_examples,\n",
    "                    i*batch_size, (i+1)*batch_size, \n",
    "                    max_seq_length, \n",
    "                    tokenizer, \n",
    "                    _result))\n",
    "        pr.start()\n",
    "        processes.append(pr)\n",
    "    pr = multiprocessing.Process(\n",
    "        target = convert_examples_to_features_multi_pipe,\n",
    "        args = (train_examples, \n",
    "                batch_size*n_cpu, \n",
    "                num_train_examples, \n",
    "                max_seq_length,  \n",
    "                tokenizer, _result))\n",
    "    \n",
    "    pr.start()\n",
    "    processes.append(pr)\n",
    "    for pr in processes: \n",
    "        pr.join()\n",
    "    train_features = list(_result)\n",
    "    return train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features = convert_examples_to_features_multi(\n",
    "    train_examples, \n",
    "    args['max_seq_length'], \n",
    "    tokenizer, n_cpu=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41850"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
